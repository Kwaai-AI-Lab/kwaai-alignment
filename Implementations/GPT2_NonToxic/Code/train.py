from modules import *
from arguments import args
from dataset import build_dataset, collator



if __name__ == '__main__':
    config = PPOConfig(
        model_name=args.model_name,
        learning_rate=args.learning_rate,
        log_with="wandb",
    )
    
    # setting the seed
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)

    wandb.init("") # access token for weights and biases

    dataset = build_dataset()

    # This is the model we are going to fine-tune with PPO
    model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)
    # This is the reference model (frozen) for the KL divergence
    ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)

    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token

    ppo_trainer = PPOTrainer(config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)

    device = ppo_trainer.accelerator.device
    if ppo_trainer.accelerator.num_processes == 1:
        device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug

    # This is the reward model: a "positive" (e.g. a positive review) response will be given a high reward, a "negative" response will be given a low reward
    sentiment_pipe = pipeline("text-classification", model=args.classfication_model_name, device=device)


    # Print some examples of sentiments generated by the reward model
    sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": args.batch_size}

    output_min_length = args.output_min_length
    output_max_length = args.output_max_length
    output_length_sampler = LengthSampler(output_min_length, output_max_length)

    # The configuration to generate responses (trajectories)
    response_generation_kwargs = {
        "min_length": -1,
        "top_k": 0.0,
        "top_p": 1.0,
        "do_sample": True,
        "pad_token_id": tokenizer.eos_token_id,
    }
    

    for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):

        query_tensors = batch["input_ids"]

        #### Phase 1: Get trajectories from the offline policy
        # In this case we are only generating the responses, but not computing the log probabilities, which will be computed internally by the PPOTrainer.
        response_tensors = []
        for query in query_tensors:
            gen_len = output_length_sampler()
            response_generation_kwargs["max_new_tokens"] = gen_len # Number of tokens to generate (chosen randomly)
            response = ppo_trainer.generate(query, **response_generation_kwargs) # It returns the (query + response) tokens
            response_tensors.append(response.squeeze()[-gen_len:]) # Only take the tokens corresponding to the generated response (remove the prompt/query from the beginning)
        batch["response"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]

        #### Phase 1: Compute rewards
        # Join the query (prompt) + response (generated tokens)
        texts = [q + r for q, r in zip(batch["query"], batch["response"])]
        # Compute the reward for each of the texts (query + response)
        # shape: A list of dictionaries with two keys: Non-Toxic and Toxic. We are interested in the Non-Toxic score. This will be our reward.
        pipe_outputs = sentiment_pipe(texts, **sent_kwargs) 
        # The reward for each text is the score (logit) corresponding to the Non-Toxic class. 
        # shape: A list of scalars, one for each generated response. 
        # It means we assign the reward to the whole response (not to each token).
        rewards = [torch.tensor(output[0]["score"]) for output in pipe_outputs]

        #### Phase 1 + Phase 2: calculate the logprobs and then run the PPO update
        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
        
        ppo_trainer.log_stats(stats, batch, rewards)
        
        # postfix for tqdm
#         tqdm.set_postfix({'Policy Loss': stats['policy_loss'], 'Value Loss': stats['value_loss']})

    model.save_pretrained("gpt2-non-toxic", push_to_hub=False)
    tokenizer.save_pretrained("gpt2_tokenizer", push_to_hub=False)
