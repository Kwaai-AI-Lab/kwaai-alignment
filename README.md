# Kwaai Alignment Research Lab

### :open_file_folder: Google Drive: [Kwaai Alignment Research Lab Drive](https://drive.google.com/drive/folders/1GHGNe58ARRyoc9wSVKIcwUPovjnoFbgi?usp=sharing)
### Github: [Kwaai Alignment GitHub](https://github.com/Kwaai-AI-Lab/kwaai-alignment)
### :handshake: Meetings: TBD

---

## FAQ:

### What are the goals of this group?
Our current goal this year is to produce a survey paper in alignment research and run small experiments.

### How can I contribute to this group?
Right now, we’re working on the survey and small experiments. If you’d like to help out with either, reach out and we can get you started. Also, feel free to check the above Google Drive (Request access)

### What if I don’t know anything about alignment?
That's okay! We welcome all different backgrounds. If you are interested in this area and willing to help, then that is great.

### Contributions: I want to run a small experiment I read in a paper, can I add my code and findings somewhere?
Yes, create a new branch, add your changes using the below format to `/Implementations`, and open a PR.

#### Folder Structure
Please create a folder named `{experiment_name}` inside the `/Implementations` directory. The structure should look like this:

```
/Implementations
│
└───{/experiment_name}
│   │   README.md
│   │   relevant_paper_1.pdf
│   │   relevant_paper_1.pdf
│   │
│   └───code
│       │   script_1.py
│       │   script_2.py
│       │   ...

```

Feel free to add more specific instructions or scripts based on the particular experiment and its requirements.

## Useful Reads to Get Up to Speed:
- **Research Priorities for Robust and Beneficial AI**: [Read here](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/2577)
- **Concrete Problems in AI Safety (2016)**: [Read here](https://arxiv.org/pdf/1606.06565)
- **Unsolved Problems in ML Safety (2022)**: [Read here](https://arxiv.org/pdf/2109.13916)

---

## Alignment Forums:
- [Alignment Forum](https://www.alignmentforum.org/)
- [LessWrong AI Tag](https://www.lesswrong.com/tag/ai)

---

## Institutions:
- [Center for AI Safety](https://www.safe.ai/)
- [Future of Life Institute](https://futureoflife.org/)
- [AI Safety Institute](https://www.aisi.gov.uk/)
